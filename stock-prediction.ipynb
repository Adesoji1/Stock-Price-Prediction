{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard","accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Install Dependencies\n!pip install pandas sklearn numerapi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyVz68zo2xVg","outputId":"1a803d97-3d86-4b25-9b42-65935c4279ee","execution":{"iopub.status.busy":"2023-02-23T22:09:12.045903Z","iopub.execute_input":"2023-02-23T22:09:12.046701Z","iopub.status.idle":"2023-02-23T22:09:15.584297Z","shell.execute_reply.started":"2023-02-23T22:09:12.046556Z","shell.execute_reply":"2023-02-23T22:09:15.583155Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\nCollecting sklearn\n  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[18 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m If the previous advice does not cover your use case, feel free to report it at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numerapi\nimport sklearn.linear_model\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"zHkLzO4K5fes","execution":{"iopub.status.busy":"2023-02-19T09:22:29.917373Z","iopub.execute_input":"2023-02-19T09:22:29.917862Z","iopub.status.idle":"2023-02-19T09:22:30.319489Z","shell.execute_reply.started":"2023-02-19T09:22:29.917821Z","shell.execute_reply":"2023-02-19T09:22:30.318231Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"training_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\")\ntraining_data.head()\n# training_data.to_csv('traindata.csv')\nimport pandas as pd\nimport numerapi\nimport sklearn.linear_model\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"EyBO-DPm5mK-","outputId":"3e3fb65e-e687-4e7a-c9c1-2b38a60017e3","execution":{"iopub.status.busy":"2023-02-19T09:22:35.745534Z","iopub.execute_input":"2023-02-19T09:22:35.746838Z","iopub.status.idle":"2023-02-19T09:23:08.503194Z","shell.execute_reply.started":"2023-02-19T09:22:35.746782Z","shell.execute_reply":"2023-02-19T09:23:08.501992Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# download the latest tournament dataset (takes around 30s)\ntournament_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\")\ntournament_data.head()\n#tournament_data.to_csv('testdata.csv')\n     ","metadata":{"id":"Ds-6xb-u53PQ","execution":{"iopub.status.busy":"2023-02-19T09:23:08.505432Z","iopub.execute_input":"2023-02-19T09:23:08.505955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = pd.read_csv('/kaggle/working/traindata.csv')\ntournament_data = pd.read_csv('/kaggle/working/testdata.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:13:11.060426Z","iopub.execute_input":"2023-02-19T01:13:11.060937Z","iopub.status.idle":"2023-02-19T01:15:17.092441Z","shell.execute_reply.started":"2023-02-19T01:13:11.060905Z","shell.execute_reply":"2023-02-19T01:15:17.086634Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# find only the feature columns\nfeature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\nfeature_cols","metadata":{"id":"GNxUXgeZ6B5V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select those columns out of the training dataset\ntraining_features = training_data[feature_cols]\ntraining_features\n     ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"id":"VNs0m3eg6NTE","outputId":"e50db0ed-0ec9-4225-fc48-5607bcfbe947","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:17.709308Z","iopub.execute_input":"2023-02-19T01:15:17.709712Z","iopub.status.idle":"2023-02-19T01:15:17.717489Z","shell.execute_reply.started":"2023-02-19T01:15:17.709678Z","shell.execute_reply":"2023-02-19T01:15:17.716224Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(501808, 315)"},"metadata":{}}]},{"cell_type":"code","source":"training_data.target\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:17.718952Z","iopub.execute_input":"2023-02-19T01:15:17.719336Z","iopub.status.idle":"2023-02-19T01:15:17.734642Z","shell.execute_reply.started":"2023-02-19T01:15:17.719304Z","shell.execute_reply":"2023-02-19T01:15:17.733310Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0         0.50\n1         0.25\n2         0.25\n3         0.25\n4         0.75\n          ... \n501803    0.50\n501804    0.75\n501805    0.25\n501806    0.50\n501807    0.50\nName: target, Length: 501808, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"training_data.target.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:17.736671Z","iopub.execute_input":"2023-02-19T01:15:17.737057Z","iopub.status.idle":"2023-02-19T01:15:17.745079Z","shell.execute_reply.started":"2023-02-19T01:15:17.737016Z","shell.execute_reply":"2023-02-19T01:15:17.743864Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(501808,)"},"metadata":{}}]},{"cell_type":"code","source":"training_features.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:17.746552Z","iopub.execute_input":"2023-02-19T01:15:17.747038Z","iopub.status.idle":"2023-02-19T01:15:17.758527Z","shell.execute_reply.started":"2023-02-19T01:15:17.746990Z","shell.execute_reply":"2023-02-19T01:15:17.757465Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(501808, 310)"},"metadata":{}}]},{"cell_type":"code","source":"train = training_data[training_data['data_type']=='train']\nval = training_data[training_data['data_type']=='validation']\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:46.910212Z","iopub.execute_input":"2023-02-19T01:15:46.910704Z","iopub.status.idle":"2023-02-19T01:15:47.896102Z","shell.execute_reply.started":"2023-02-19T01:15:46.910664Z","shell.execute_reply":"2023-02-19T01:15:47.894778Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train.target","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:48.790314Z","iopub.execute_input":"2023-02-19T01:15:48.790815Z","iopub.status.idle":"2023-02-19T01:15:48.800606Z","shell.execute_reply.started":"2023-02-19T01:15:48.790774Z","shell.execute_reply":"2023-02-19T01:15:48.799502Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0         0.50\n1         0.25\n2         0.25\n3         0.25\n4         0.75\n          ... \n501803    0.50\n501804    0.75\n501805    0.25\n501806    0.50\n501807    0.50\nName: target, Length: 501808, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"train.target.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:53.309924Z","iopub.execute_input":"2023-02-19T01:15:53.310327Z","iopub.status.idle":"2023-02-19T01:15:53.318855Z","shell.execute_reply.started":"2023-02-19T01:15:53.310297Z","shell.execute_reply":"2023-02-19T01:15:53.317545Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(501808,)"},"metadata":{}}]},{"cell_type":"code","source":"train_features = training_data[feature_cols]","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:54.779509Z","iopub.execute_input":"2023-02-19T01:15:54.779929Z","iopub.status.idle":"2023-02-19T01:15:55.239414Z","shell.execute_reply.started":"2023-02-19T01:15:54.779894Z","shell.execute_reply":"2023-02-19T01:15:55.237982Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_features.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:17.794240Z","iopub.status.idle":"2023-02-19T01:15:17.795133Z","shell.execute_reply.started":"2023-02-19T01:15:17.794895Z","shell.execute_reply":"2023-02-19T01:15:17.794926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select those columns out of the training dataset\ntraining_features = training_data[feature_cols]\ntraining_features","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:58.300882Z","iopub.execute_input":"2023-02-19T01:15:58.301813Z","iopub.status.idle":"2023-02-19T01:15:58.853689Z","shell.execute_reply.started":"2023-02-19T01:15:58.301755Z","shell.execute_reply":"2023-02-19T01:15:58.852655Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"        feature_intelligence1  feature_intelligence2  feature_intelligence3  \\\n0                        0.00                   0.50                   0.25   \n1                        0.00                   0.00                   0.00   \n2                        0.25                   0.50                   0.25   \n3                        1.00                   0.00                   0.00   \n4                        0.25                   0.25                   0.25   \n...                       ...                    ...                    ...   \n501803                   0.50                   0.50                   0.25   \n501804                   1.00                   0.00                   0.00   \n501805                   0.75                   0.50                   0.50   \n501806                   0.25                   0.25                   0.25   \n501807                   0.75                   0.50                   0.50   \n\n        feature_intelligence4  feature_intelligence5  feature_intelligence6  \\\n0                        0.00                   0.50                   0.25   \n1                        0.25                   0.50                   0.00   \n2                        0.25                   1.00                   0.75   \n3                        0.50                   0.50                   0.25   \n4                        0.25                   0.00                   0.25   \n...                       ...                    ...                    ...   \n501803                   0.00                   0.00                   0.50   \n501804                   1.00                   0.50                   0.75   \n501805                   0.50                   0.25                   0.50   \n501806                   0.50                   0.00                   1.00   \n501807                   0.75                   0.75                   0.00   \n\n        feature_intelligence7  feature_intelligence8  feature_intelligence9  \\\n0                        0.25                   0.25                   0.75   \n1                        0.00                   0.25                   0.50   \n2                        0.75                   0.25                   0.00   \n3                        0.25                   0.75                   0.25   \n4                        0.50                   0.25                   0.25   \n...                       ...                    ...                    ...   \n501803                   0.75                   0.00                   0.75   \n501804                   0.75                   1.00                   0.00   \n501805                   0.25                   0.50                   0.25   \n501806                   1.00                   0.50                   0.25   \n501807                   0.00                   0.75                   0.00   \n\n        feature_intelligence10  ...  feature_wisdom37  feature_wisdom38  \\\n0                         0.75  ...              1.00              1.00   \n1                         0.50  ...              0.75              1.00   \n2                         0.25  ...              0.50              0.25   \n3                         0.50  ...              0.75              1.00   \n4                         0.50  ...              0.50              0.75   \n...                        ...  ...               ...               ...   \n501803                    0.75  ...              0.75              0.50   \n501804                    0.00  ...              1.00              1.00   \n501805                    0.25  ...              1.00              1.00   \n501806                    0.25  ...              0.50              0.75   \n501807                    0.00  ...              0.50              0.50   \n\n        feature_wisdom39  feature_wisdom40  feature_wisdom41  \\\n0                   1.00              0.75              0.50   \n1                   1.00              0.00              0.00   \n2                   0.50              0.00              0.00   \n3                   1.00              0.75              0.75   \n4                   0.75              0.25              0.50   \n...                  ...               ...               ...   \n501803              0.50              0.75              0.50   \n501804              1.00              1.00              1.00   \n501805              0.75              0.25              1.00   \n501806              0.75              0.75              0.75   \n501807              0.50              0.25              0.50   \n\n        feature_wisdom42  feature_wisdom43  feature_wisdom44  \\\n0                   0.75              0.50              1.00   \n1                   0.75              0.25              0.00   \n2                   0.50              1.00              0.00   \n3                   1.00              1.00              0.75   \n4                   0.75              0.00              0.50   \n...                  ...               ...               ...   \n501803              0.50              0.75              0.25   \n501804              1.00              0.00              0.75   \n501805              1.00              1.00              0.25   \n501806              0.75              0.50              0.50   \n501807              0.75              1.00              0.25   \n\n        feature_wisdom45  feature_wisdom46  \n0                   0.50              0.75  \n1                   0.25              1.00  \n2                   0.25              0.75  \n3                   1.00              1.00  \n4                   0.25              0.75  \n...                  ...               ...  \n501803              0.25              0.25  \n501804              1.00              1.00  \n501805              0.00              0.00  \n501806              0.25              0.75  \n501807              0.75              0.50  \n\n[501808 rows x 310 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_intelligence1</th>\n      <th>feature_intelligence2</th>\n      <th>feature_intelligence3</th>\n      <th>feature_intelligence4</th>\n      <th>feature_intelligence5</th>\n      <th>feature_intelligence6</th>\n      <th>feature_intelligence7</th>\n      <th>feature_intelligence8</th>\n      <th>feature_intelligence9</th>\n      <th>feature_intelligence10</th>\n      <th>...</th>\n      <th>feature_wisdom37</th>\n      <th>feature_wisdom38</th>\n      <th>feature_wisdom39</th>\n      <th>feature_wisdom40</th>\n      <th>feature_wisdom41</th>\n      <th>feature_wisdom42</th>\n      <th>feature_wisdom43</th>\n      <th>feature_wisdom44</th>\n      <th>feature_wisdom45</th>\n      <th>feature_wisdom46</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>1.00</td>\n      <td>0.50</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>...</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>...</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>501803</th>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>501804</th>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>501805</th>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>501806</th>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>501807</th>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.50</td>\n    </tr>\n  </tbody>\n</table>\n<p>501808 rows × 310 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train.iloc[:,3:-1]\nX_train\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:17.798758Z","iopub.status.idle":"2023-02-19T01:15:17.799842Z","shell.execute_reply.started":"2023-02-19T01:15:17.799607Z","shell.execute_reply":"2023-02-19T01:15:17.799630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:17.801424Z","iopub.status.idle":"2023-02-19T01:15:17.801846Z","shell.execute_reply.started":"2023-02-19T01:15:17.801644Z","shell.execute_reply":"2023-02-19T01:15:17.801664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train['target']\ny_train","metadata":{"execution":{"iopub.status.busy":"2023-02-19T01:15:17.803430Z","iopub.status.idle":"2023-02-19T01:15:17.804261Z","shell.execute_reply.started":"2023-02-19T01:15:17.804031Z","shell.execute_reply":"2023-02-19T01:15:17.804053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#using LSTM Model. we get an ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport time\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, SimpleRNN\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n# download the latest training dataset (takes around 30s)\ntraining_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\")\ntraining_data.head()\n# download the latest tournament dataset (takes around 30s)\ntournament_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\")\ntournament_data.head()\n\n# find only the feature columns\nfeature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n     \n\n# select those columns out of the training dataset\ntraining_features = training_data[feature_cols]\n\n# model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(training_features.shape[1], 1)))\n# model.add(Dropout(0.3))\n# model.add(LSTM(units=50, return_sequences=True))\n# model.add(Dropout(0.2))\n# model.add(LSTM(units=50))\n# model.add(Dropout(0.2))\n# model.add(Dense(units=1))\n# model.compile(optimizer='adam', loss=\"mean_squared_error\", metrics=[\"acc\"], run_eagerly=True)\n\nmodel = Sequential()\n\nmodel.add(LSTM(\n    input_dim=1,\n    units=50,\n    return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(\n    100,\n    return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(\n    units=1))\nmodel.add(Activation('linear'))\n\nstart = time.time()\nmodel.compile(loss='mse', optimizer='rmsprop')\nprint ('compilation time : ', time.time() - start)\n\nmodel.fit(\n    training_features,\n    training_data.target,\n    batch_size=128,\n    epochs=2)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T10:30:18.969389Z","iopub.execute_input":"2023-02-19T10:30:18.969871Z","iopub.status.idle":"2023-02-19T12:10:37.211446Z","shell.execute_reply.started":"2023-02-19T10:30:18.969830Z","shell.execute_reply":"2023-02-19T12:10:37.209116Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"compilation time :  0.00522303581237793\nEpoch 1/2\n3921/3921 [==============================] - 2928s 746ms/step - loss: 0.0510\nEpoch 2/2\n3921/3921 [==============================] - 2940s 750ms/step - loss: 0.0501\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f566c2c8ed0>"},"metadata":{}}]},{"cell_type":"code","source":"# select the feature columns from the tournament data\nlive_features = tournament_data[feature_cols]","metadata":{"execution":{"iopub.status.busy":"2023-02-19T12:13:31.548445Z","iopub.execute_input":"2023-02-19T12:13:31.549357Z","iopub.status.idle":"2023-02-19T12:13:46.379961Z","shell.execute_reply.started":"2023-02-19T12:13:31.549267Z","shell.execute_reply":"2023-02-19T12:13:46.378540Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.layers import Activation\nfrom sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n\n# Load the pre-processed and obfuscated dataset\ntrain_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\")\ntest_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\")\n\n\n# Check if data is empty\nif len(train_data) == 0:\n    print(\"Train data is empty\")\nif len(test_data) == 0:\n    print(\"Test data is empty\")\n\n# Split the train data into train and validation sets\nX_train = train_data.filter(regex='^feature', axis=1).values\ny_train = train_data['target'].values.reshape(-1, 1)\n\n# Scale the data\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\ny_train_scaled = scaler.fit_transform(y_train)\n\n# Reshape the data for LSTM input\nX_train_reshaped = X_train_scaled.reshape(X_train.shape[0], X_train.shape[1], 1)\n\n# Train the model\nmodel = Sequential()\nmodel.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], 1)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(units=100, return_sequences=True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(units=150))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(units=200))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=1))\nmodel.add(Activation('linear'))\nmodel.compile(optimizer='rmsprop', loss='mean_squared_error')\n\n# Fit the model to the training data\nmodel.fit(X_train_reshaped, y_train_scaled, epochs=1, batch_size=32)\n\n# Predict on the validation set and compute metrics\n# select the feature columns from the tournament data\nX_test = test_data.filter(regex='^feature', axis=1).values\nX_test_scaled = scaler.transform(X_test)\nX_test_reshaped = X_test_scaled.reshape(X_test.shape[0], X_test.shape[1], 1)\n\n# predict the target on the live features\ny_pred_val_scaled = model.predict(X_test_reshaped)\ny_pred_val = scaler.inverse_transform(y_pred_val_scaled)\n\n# predictions must have an `id` column and a `prediction` column\npredictions_df = test_data[\"id\"].to_frame()\npredictions_df[\"prediction\"] = y_pred_val\npredictions_df.head()\n\n# Compute evaluation metrics\nmse = mean_squared_error(train_data['target'], y_pred_val)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(train_data['target'], y_pred_val)\nmape = mean_absolute_percentage_error(train_data['target'], y_pred_val)\nr2 = r2_score(train_data['target'], y_pred_val)\n\n\nprint('Metrics on validation set:')\nprint('MSE:', mse)\nprint('RMSE:', rmse)\nprint('MAE:', mae)\nprint('MAPE:', mape)\nprint('R-squared:', r2)\n\nprint(\"THE END\")","metadata":{"execution":{"iopub.status.busy":"2023-02-21T04:08:30.082436Z","iopub.execute_input":"2023-02-21T04:08:30.082816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Random forest regression\nrf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nrf.fit(training_features, train_df.target)\nrf_pred = rf.predict(live_features)\n\n# Support vector regression\nsvm = SVR(kernel='rbf')\nsvm.fit(training_features, train_df.target)\nsvm_pred = svm.predict(live_features)\n\n# Evaluate the performance of the models\nprint('Random Forest Regression')\nprint('MAE:', mean_absolute_error(train_df.target, rf_pred))\nprint('RMSE:', mean_squared_error(train_df.target, rf_pred, squared=False))\nprint('MSE:', mean_squared_error(train_df.target, rf_pred))\nprint('R^2:', r2_score(train_df.target, rf_pred))\nprint()\nprint('Support Vector Regression')\nprint('MAE:', mean_absolute_error(train_df.target, svm_pred))\nprint('RMSE:', mean_squared_error(train_df.target, svm_pred, squared=False))\nprint('MSE:', mean_squared_error(train_df.target, svm_pred))\nprint('R^2:', r2_score(train_df.target, svm_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# load preprocessed data that is scaled and standardized\ntrain_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\")\ntest_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\")\n\n# split data into X and y\ntrain_X = train_data.drop(['id', 'era', 'data_type', 'target'], axis=1)\ntrain_y = train_data['target']\ntest_X = test_data.drop(['id', 'era', 'data_type', 'target'], axis=1)\ntest_y = test_data['target']\n\n# random forest regression\nrf = RandomForestRegressor(n_estimators=100, random_state=42,n_jobs=-1)\nrf.fit(train_X, train_y)\nrf_pred = rf.predict(test_X)\nrf_mae = mean_absolute_error(test_y, rf_pred)\nrf_rmse = np.sqrt(mean_squared_error(test_y, rf_pred))\nrf_mse = mean_squared_error(test_y, rf_pred)\nrf_r2 = r2_score(test_y, rf_pred)\n\n# SVM regression\nsvm = SVR()\nsvm.fit(train_X, train_y)\nsvm_pred = svm.predict(test_X)\nsvm_mae = mean_absolute_error(test_y, svm_pred)\nsvm_rmse = np.sqrt(mean_squared_error(test_y, svm_pred))\nsvm_mse = mean_squared_error(test_y, svm_pred)\nsvm_r2 = r2_score(test_y, svm_pred)\n\nprint(\"Random Forest Regression:\")\nprint(\"MAE:\", rf_mae)\nprint(\"RMSE:\", rf_rmse)\nprint(\"MSE:\", rf_mse)\nprint(\"R^2 Score:\", rf_r2)\n\nprint(\"\\nSVM Regression:\")\nprint(\"MAE:\", svm_mae)\nprint(\"RMSE:\", svm_rmse)\nprint(\"MSE:\", svm_mse)\nprint(\"R^2 Score:\", svm_r2)\n\n# split data into X and y\ntrain_X = train_data.drop(['id', 'era', 'data_type', 'target'], axis=1)\ntrain_y = train_data['target']\ntest_X = test_data.drop(['id', 'era', 'data_type', 'target'], axis=1)\ntest_y = test_data['target']\n\n# random forest regression\nrf = RandomForestRegressor(n_estimators=100, random_state=42,n_jobs=-1)\nrf.fit(train_X, train_y)\nrf_pred = rf.predict(test_X)\nrf_mae = mean_absolute_error(test_y, rf_pred)\nrf_rmse = np.sqrt(mean_squared_error(test_y, rf_pred))\nrf_mse = mean_squared_error(test_y, rf_pred)\nrf_r2 = r2_score(test_y, rf_pred)\n\n# SVM regression\nsvm = SVR()\nsvm.fit(train_X, train_y)\nsvm_pred = svm.predict(test_X)\nsvm_mae = mean_absolute_error(test_y, svm_pred)\nsvm_rmse = np.sqrt(mean_squared_error(test_y, svm_pred))\nsvm_mse = mean_squared_error(test_y, svm_pred)\nsvm_r2 = r2_score(test_y, svm_pred)\n\nprint(\"Random Forest Regression:\")\nprint(\"MAE:\", rf_mae)\nprint(\"RMSE:\", rf_rmse)\nprint(\"MSE:\", rf_mse)\nprint(\"R^2 Score:\", rf_r2)\n\nprint(\"\\nSVM Regression:\")\nprint(\"MAE:\", svm_mae)\nprint(\"RMSE:\", svm_rmse)\nprint(\"MSE:\", svm_mse)\nprint(\"R^2 Score:\", svm_r2)\n# split data into X and y\ntrain_X = train_data.drop(['id', 'era', 'data_type', 'target'], axis=1)\ntrain_y = train_data['target']\ntest_X = test_data.drop(['id', 'era', 'data_type', 'target'], axis=1)\ntest_y = test_data['target']\n\n# random forest regression\nrf = RandomForestRegressor(n_estimators=100, random_state=42,n_jobs=-1)\nrf.fit(train_X, train_y)\nrf_pred = rf.predict(test_X)\nrf_mae = mean_absolute_error(test_y, rf_pred)\nrf_rmse = np.sqrt(mean_squared_error(test_y, rf_pred))\nrf_mse = mean_squared_error(test_y, rf_pred)\nrf_r2 = r2_score(test_y, rf_pred)\n\n# SVM regression\nsvm = SVR()\nsvm.fit(train_X, train_y)\nsvm_pred = svm.predict(test_X)\nsvm_mae = mean_absolute_error(test_y, svm_pred)\nsvm_rmse = np.sqrt(mean_squared_error(test_y, svm_pred))\nsvm_mse = mean_squared_error(test_y, svm_pred)\nsvm_r2 = r2_score(test_y, svm_pred)\n\nprint(\"Random Forest Regression:\")\nprint(\"MAE:\", rf_mae)\nprint(\"RMSE:\", rf_rmse)\nprint(\"MSE:\", rf_mse)\nprint(\"R^2 Score:\", rf_r2)\n\nprint(\"\\nSVM Regression:\")\nprint(\"MAE:\", svm_mae)\nprint(\"RMSE:\", svm_rmse)\nprint(\"MSE:\", svm_mse)\nprint(\"R^2 Score:\", svm_r2)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T10:01:22.756931Z","iopub.execute_input":"2023-02-23T10:01:22.757306Z"},"trusted":true},"execution_count":null,"outputs":[]}]}